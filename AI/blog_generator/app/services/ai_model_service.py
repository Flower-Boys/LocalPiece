import torch
from pathlib import Path
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM
from ultralytics import YOLO
import torchvision.models as models
import torchvision.transforms as transforms
from typing import Dict
import re

from app.config import MODELS_DIR, DATA_DIR

class AIModelService:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(AIModelService, cls).__new__(cls)
            # ğŸ‘ˆ [ìˆ˜ì • 1] GPUê°€ ì—†ì–´ë„ í•­ìƒ CPUë¡œ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ë„ë¡ ë³€ê²½
            cls._instance._initialize_models()
        return cls._instance

    def _initialize_models(self):
        self.initialized = True
        # ğŸ‘ˆ [ìˆ˜ì • 2] ì‚¬ìš©í•  ì¥ì¹˜ë¥¼ 'cpu'ë¡œ ëª…í™•í•˜ê²Œ ì§€ì •
        self.device = "cpu"
        print(f"AI ëª¨ë¸ì„ {self.device} í™˜ê²½ì—ì„œ ë¡œë”©í•©ë‹ˆë‹¤...")
        
        # --- 1. ì‹œê° ë¶„ì„ê°€ ê·¸ë£¹ (YOLO, Places365, BLIP) ---
        print("  - ì‹œê° ë¶„ì„ê°€ ë¡œë”© ì¤‘...")
        
        # YOLO ëª¨ë¸ ë¡œë”©
        self.yolo_model = YOLO(MODELS_DIR / "yolov8n.pt")
        self.yolo_model.to(self.device) # ëª¨ë¸ì„ CPUë¡œ ì´ë™

        # Places365 ëª¨ë¸ ë¡œë”©
        places_model_path = MODELS_DIR / "resnet18_places365.pth.tar"
        self.places_model = models.resnet18(num_classes=365)
        # ğŸ‘ˆ [ìˆ˜ì • 3] CPUì—ì„œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì˜¤ë„ë¡ map_location ì„¤ì •
        checkpoint = torch.load(places_model_path, map_location=self.device)
        state_dict = {str.replace(k, 'module.', ''): v for k, v in checkpoint['state_dict'].items()}
        self.places_model.load_state_dict(state_dict)
        self.places_model.to(self.device) # ëª¨ë¸ì„ CPUë¡œ ì´ë™
        self.places_model.eval()
        
        with open(DATA_DIR / "categories_places365.txt") as f:
            self.places_categories = [line.strip().split(' ')[0][3:] for line in f]
            
        self.places_transform = transforms.Compose([
            transforms.Resize((256, 256)), transforms.CenterCrop(224),
            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        # BLIP ëª¨ë¸ ë¡œë”©
        blip_model_id = "Salesforce/blip-image-captioning-base"
        self.blip_processor = BlipProcessor.from_pretrained(blip_model_id, use_fast=True)
        self.blip_model = BlipForConditionalGeneration.from_pretrained(blip_model_id).to(self.device)


        # --- 2. í•œê¸€ ì‘ê°€ (EXAONE) ---
        print("  - í•œê¸€ ì‘ê°€(EXAONE-4.0-1.2B) ë¡œë”© ì¤‘...")
        exaone_model_id = "LGAI-EXAONE/EXAONE-4.0-1.2B"
        
        self.exaone_tokenizer = AutoTokenizer.from_pretrained(exaone_model_id)
        # ğŸ‘ˆ [ìˆ˜ì • 4] CPU í™˜ê²½ì— ë§ê²Œ GPU ê´€ë ¨ ì˜µì…˜(torch_dtype, device_map) ì œê±°
        self.exaone_model = AutoModelForCausalLM.from_pretrained(exaone_model_id).to(self.device)
        
        print("AI ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")

    def analyze_image_to_keywords(self, image_path: Path) -> Dict:
        if not self.initialized: return {"error": "Model not initialized"}
        
        img = Image.open(image_path).convert("RGB")
        
        # ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•´ í…ì„œë¥¼ CPUë¡œ ë³´ëƒ…ë‹ˆë‹¤.
        img_tensor = self.places_transform(img).unsqueeze(0).to(self.device)
        
        # YOLO ì˜ˆì¸¡
        yolo_results = self.yolo_model.predict(img, verbose=False)[0]
        yolo_objects = list(set([yolo_results.names[int(c)] for c in yolo_results.boxes.cls]))
        
        # Places365 ì˜ˆì¸¡
        with torch.no_grad():
            output = self.places_model(img_tensor)
        _, pred = output.topk(1, 1, True, True)
        place_type = self.places_categories[pred[0][0]]
        
        # BLIP ì˜ˆì¸¡
        inputs = self.blip_processor(images=img, return_tensors="pt").to(self.device)
        out = self.blip_model.generate(**inputs, max_new_tokens=20)
        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
        
        blip_keywords = [word for word in re.split(r'\W+', caption.lower()) if len(word) > 2 and word not in ['the', 'and', 'with', 'are']]
        return { "yolo_objects": yolo_objects[:3], "place_type": [place_type], "blip_keywords": list(set(blip_keywords))[:5] }


    def generate_korean_text_from_keywords(self, prompt: str) -> str:
        if not self.initialized: return "ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        messages = [{"role": "user", "content": prompt}]
        input_ids = self.exaone_tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.device) # ğŸ‘ˆ [ìˆ˜ì • 5] ìƒì„±ëœ í…ì„œë¥¼ CPUë¡œ ì´ë™
        
        output = self.exaone_model.generate(
            input_ids,
            max_new_tokens=150,
            do_sample=True,
            temperature=0.7
        )
        
        response_text = self.exaone_tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True)
        return response_text

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ì—¬ ë‹¤ë¥¸ íŒŒì¼ì—ì„œ 'ai_models'ë¥¼ importí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
ai_models = AIModelService()